---
title: "DSCI 522 Lecture 7"
subtitle: "Data Analysis Pipeline and GNU Make"
author: "Sky Sheng"
execute:
  eval: false
format:
  revealjs:
    theme: default
    transition: slide
    slide-number: true
    chalkboard: true
    logo: ../images/mds-hex-sticker.png
---

# üêõ Recap: bug of last week

```yaml
services:
  # run jupyter notebook inside jupyter 
  jupyter-notebook:
    image:  skysheng7/dsci522:681bccb
    ports:
      - "8888:8888"
    volumes:
      - .:/home/jovyan
    deploy:
      resources:
        limits:
          memory: 5G
    platform: linux/amd64
```

# Data Analysis Pipeline 

![](../images/otter_workflow.png){width=120 fig-align="center"}

<p style="position: fixed; bottom: -40px; right: 5px; font-size: 18px; color: gray; z-index: 1000;">Image generated by OpenAI GPT-5</p>


# What is a data analysis pipeline?

::: {.incremental}
* A **pipeline** is a sequence of data processing steps
* Each step takes input and produces output
* Steps are connected: output of one step becomes input to the next
:::

## Shell script demo

1. Go to this [repository](https://github.com/ttimbers/data_analysis_pipeline_practice) and click the green "Use this template" button to get your own copy of the `data_analysis_pipeline_practice` repository.

2. Clone the repository and `cd` into the folder.

```bash
git clone <repository_url>
cd <repository_name>
```

3. Install and ativate the conda environment (linux-64, osx-arm64, osx-64, win-64 OS)

```bash
conda-lock install --name da-pipeline-make conda-lock.yml
conda activate da-pipeline-make
```

## Alternative way

::: {.smaller style="font-size: 0.8em;"}
1. Create a `docker-compose.yml` file locally in your repository. 

2. Copy and paste the following code into the `docker-compose.yml` file.

```yaml
services:
  # run jupyter notebook inside jupyter 
  jupyter-notebook:
    image: skysheng7/dsci522:681bccb
    ports:
      - "8888:8888"
    volumes:
      - .:/home/jovyan
    deploy:
      resources:
        limits:
          memory: 5G
    platform: linux/amd64
```

3. Run the command in your terminal: 

```bash
docker compose up
```
:::

## Command line non-interactive scripts üìó

::: {.smaller style="font-size: 0.9em;"}
4. Open your terminal and run this python script, it reads in a text file, counts the words in this text file, and outputs a data file

```bash
python scripts/wordcount.py \
    --input_file=data/isles.txt \
    --output_file=results/isles.dat
```

5. Check out the first few rows of the output data file: 

```bash
head results/isles.dat
```

6. Now we have another script that reads in a data file and save a plot of the 10 most frequently occurring words:

```bash
python scripts/plotcount.py \
    --input_file=results/isles.dat \
    --output_file=results/figure/isles.png
```

:::

## Pipeline

::: {.incremental}

`scripts/wordcount.py`: 

1. Read a data file.
2. Perform an analysis on this data file.
3. Write the analysis results to a new file.

`scripts/plotcount.py`: 

4. Plot a graph of the analysis results
:::

## Create a shell script to run the pipeline

1. Create a shell script called `run_pip.sh` at root directory

```bash
# perform wordcout on novels
python scripts/wordcount.py \
    --input_file=data/isles.txt \
    --output_file=results/isles.dat

# create plots
python scripts/plotcount.py \
    --input_file=results/isles.dat \
    --output_file=results/figure/isles.png
```

2. In your terminal, run: 

```bash
bash run_pip.sh
```

## Complete shell script to run all 4 books

```bash
# run_all.sh
# Tiffany Timbers, Nov 2018

# This driver script completes the textual analysis of
# 3 novels and creates figures on the 10 most frequently
# occuring words from each of the 3 novels. This script
# takes no arguments.

# example usage:
# bash run_all.sh

# count the words
python scripts/wordcount.py --input_file=data/isles.txt --output_file=results/isles.dat
python scripts/wordcount.py --input_file=data/abyss.txt --output_file=results/abyss.dat
python scripts/wordcount.py --input_file=data/last.txt --output_file=results/last.dat
python scripts/wordcount.py --input_file=data/sierra.txt --output_file=results/sierra.dat

# create the plots
python scripts/plotcount.py --input_file=results/isles.dat --output_file=results/figure/isles.png
python scripts/plotcount.py --input_file=results/abyss.dat --output_file=results/figure/abyss.png
python scripts/plotcount.py --input_file=results/last.dat --output_file=results/figure/last.png
python scripts/plotcount.py --input_file=results/sierra.dat --output_file=results/figure/sierra.png

# write the report
quarto render report/count_report.qmd
```

## Complete pipeline with a report: 
::: {.incremental}
::: {.smaller style="font-size: 0.9em;"}
`scripts/wordcount.py`: 

1. Read a data file.
2. Perform an analysis on this data file.
3. Write the analysis results to a new file.

`scripts/plotcount.py`: 

4. Plot a graph of the analysis results

`quarto render`

5. Use the plots we generated to create a report
:::
:::

## Your milestone 3: [Tiffany's example](https://github.com/ttimbers/breast-cancer-predictor/tree/2.0.0)  üéØ

::: {.smaller style="font-size: 0.8em;"}
```bash
# download the data
python scripts/download_data.py \
    --url="https://archive.ics.uci.edu/static/public/15/breast+cancer+wisconsin+original.zip" \
    --write-to=data/raw

# split and preprocess the data
python scripts/split_n_preprocess.py \
    --raw-data=data/raw/wdbc.data \
    --data-to=data/processed \
    --preprocessor-to=results/models \
    --seed=522

# perform exploratory data analysis
python scripts/eda.py \
    --processed-training-data=data/processed/scaled_cancer_train.csv \
    --plot-to=results/figures

# fit the breast cancer classifier
python scripts/fit_breast_cancer_classifier.py \
    --training-data=data/processed/cancer_train.csv \
    --preprocessor=results/models/cancer_preprocessor.pickle \
    --columns-to-drop=data/processed/columns_to_drop.csv \
    --pipeline-to=results/models \
    --plot-to=results/figures \
    --seed=523

# evaluate the breast cancer classifier
python scripts/evaluate_breast_cancer_predictor.py \
	--scaled-test-data=data/processed/cancer_test.csv \
	--pipeline-from=results/models/cancer_pipeline.pickle \
	--results-to=results/tables \
	--seed=524

# render the report
quarto render report/breast_cancer_predictor_report.qmd --to html
quarto render report/breast_cancer_predictor_report.qmd --to pdf
```
:::

# Bash script VS run python script directly from terminal ü§î

* Why don't I create a bash script to [update `environment.yml` file](https://github.com/skysheng7/append_version_to_environment_yml)?

# What are some limitations of bash shell script? üòµ

::: {.incremental}
* Manually deleting generated files is time-consuming
* Runs all steps every time, even when only small parts changed
:::

# Makefile saves the day! ü§©

* Individual Assignment 4: Create a Makefile to run all the scripts.
* [Description here](https://pages.github.ubc.ca/mds-2025-26/DSCI_522_dsci-workflows_students/release/ia4/ia4.html)
* [Guide here](https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/lectures/180-pipelines-make.html)

## When do you NOT want to `make clean` to delete all files? 

::: {.incremental}
* When the data you generated can not be re-created easilly, e.g., the data was generated interviewing 1000 people.
* When the data you generated cost money üí∞
  * [AI bias repo Makefile example](https://github.com/skysheng7/AI_bias_in_farming/blob/main/Makefile)
* Be careful with `rm -rf` command!!
  * ü§ñ Once upon a time, a student was talking to their favorite AI agent on auto-run mode, and said:
  * > "Can you delete this for me please ~" üòä
:::

## Never do this: `rm -rf ~`

![](../images/rm_rf.png){width=180 fig-align="center"}

<p style="position: fixed; bottom: 0px; right: 5px; font-size: 18px; color: gray; z-index: 1000;">Image generated by OpenAI GPT-5</p>

# ü§®  Are you tired of running...

```bash
conda export --from-history > environment.yml
```

```bash
python update_enviroment_yml.py --root_dir="." --env_name="ai_env" --yml_name="environment.yml"
```

```bash
conda-lock -k explicit --file environment.yml -p linux-64
```

::: {.fragment}
**ME TOO! ü§°**
:::

# Makefile to the rescue!


üëâ So I automated it using Makefile & GitHub Actions for my own work: 

* [cloud computing tutorial of using Arbutus](https://github.com/skysheng7/awp-arbutus-login.git)

::: {.callout-tip}
## Docker, your best friend in the cloud!
üê≥ Docker is actually very helpful in real-world data science projects, especially when using cloud computing resources!
:::

## Instructions

1. Clone and cd into this repository

```bash
git clone https://github.com/skysheng7/awp-arbutus-login.git
cd awp-arbutus-login
```

2. Install the dependencies using `environment.yml` file

```bash
conda env create -n arbutus
```

3. Activate the environment

```bash
conda activate arbutus
```

## Instructions

4. Install a new package

```bash
conda install click
```

5. Run the makefile

```bash
make env
```
